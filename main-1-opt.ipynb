{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065b8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ling/ws/pyWS/pacr/.venv/lib64/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from models import SimpleCNN\n",
    "import orjson\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b4d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2. PACR Loss (batch-wise)\n",
    "# -------------------------\n",
    "def pacr_loss(features, labels):\n",
    "    \"\"\"\n",
    "    features: (B, D)\n",
    "    labels:   (B,)\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    num_classes = labels.max().item() + 1\n",
    "    eps = 1e-8\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        idx = labels == c\n",
    "        if idx.sum() < 2:\n",
    "            continue\n",
    "        z = features[idx]  # (Nc, D)\n",
    "        mean = z.mean(dim=0, keepdim=True)\n",
    "        loss += ((z - mean) ** 2).sum(dim=1).mean()\n",
    "\n",
    "    return loss / (num_classes + eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a047e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device, lambda_pacr=0.0):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total = 0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, feats = model(x, return_feat=True)\n",
    "        loss_ce = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss = loss_ce\n",
    "        if lambda_pacr > 0:\n",
    "            loss_pacr = pacr_loss(feats, y)\n",
    "            loss = loss + lambda_pacr * loss_pacr\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return total_loss / total, total_correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58710a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_correct, total = 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return total_correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21956107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-14 22:24:15,487] A new study created in memory with name: no-name-f6fb24b8-c0bd-425e-9280-b6ef366d0d7c\n",
      "/home/ling/ws/pyWS/pacr/.venv/lib64/python3.14/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n",
      "[I 2026-01-14 22:26:50,278] Trial 0 finished with value: 0.3216 and parameters: {'lr': 0.0441877585642909, 'weight_decay': 0.0005406787344672163, 'optimizer': 'AdamW'}. Best is trial 0 with value: 0.3216.\n",
      "[I 2026-01-14 22:29:24,042] Trial 1 finished with value: 0.1193 and parameters: {'lr': 0.0022169363827064495, 'weight_decay': 0.002385623949933042, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.3216.\n",
      "[I 2026-01-14 22:31:58,355] Trial 2 finished with value: 0.5326 and parameters: {'lr': 0.0014076944288068435, 'weight_decay': 0.007677664379180153, 'optimizer': 'AdamW'}. Best is trial 2 with value: 0.5326.\n",
      "[I 2026-01-14 22:34:32,222] Trial 3 finished with value: 0.1012 and parameters: {'lr': 0.001276881085072189, 'weight_decay': 0.0003368665119136075, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.5326.\n",
      "[I 2026-01-14 22:37:06,048] Trial 4 finished with value: 0.2248 and parameters: {'lr': 0.0058234240636513245, 'weight_decay': 6.3009117068269e-05, 'optimizer': 'SGD'}. Best is trial 2 with value: 0.5326.\n",
      "[I 2026-01-14 22:37:12,278] Trial 5 pruned. \n",
      "[I 2026-01-14 22:37:18,418] Trial 6 pruned. \n",
      "[I 2026-01-14 22:39:52,851] Trial 7 finished with value: 0.6074 and parameters: {'lr': 0.0037880321937281748, 'weight_decay': 0.0005562206373344948, 'optimizer': 'AdamW'}. Best is trial 7 with value: 0.6074.\n",
      "[I 2026-01-14 22:42:26,565] Trial 8 finished with value: 0.5482 and parameters: {'lr': 0.0026680756984647264, 'weight_decay': 2.58356598242538e-05, 'optimizer': 'AdamW'}. Best is trial 7 with value: 0.6074.\n",
      "[I 2026-01-14 22:45:00,406] Trial 9 finished with value: 0.6495 and parameters: {'lr': 0.010406771084612782, 'weight_decay': 2.572424508783154e-05, 'optimizer': 'AdamW'}. Best is trial 9 with value: 0.6495.\n",
      "[I 2026-01-14 22:45:06,632] Trial 10 pruned. \n",
      "[I 2026-01-14 22:47:40,884] Trial 11 finished with value: 0.5499 and parameters: {'lr': 0.011345773827647352, 'weight_decay': 1.054147578888897e-05, 'optimizer': 'AdamW'}. Best is trial 9 with value: 0.6495.\n",
      "[I 2026-01-14 22:47:47,055] Trial 12 pruned. \n",
      "[I 2026-01-14 22:47:58,414] Trial 13 pruned. \n",
      "[I 2026-01-14 22:50:32,326] Trial 14 finished with value: 0.6431 and parameters: {'lr': 0.00598485783042418, 'weight_decay': 4.5511802565656163e-05, 'optimizer': 'AdamW'}. Best is trial 9 with value: 0.6495.\n",
      "[I 2026-01-14 22:50:38,522] Trial 15 pruned. \n",
      "[I 2026-01-14 22:50:44,779] Trial 16 pruned. \n",
      "[I 2026-01-14 22:50:50,945] Trial 17 pruned. \n",
      "[I 2026-01-14 22:50:57,148] Trial 18 pruned. \n",
      "[I 2026-01-14 22:51:08,430] Trial 19 pruned. \n",
      "[I 2026-01-14 22:51:14,573] Trial 20 pruned. \n",
      "[I 2026-01-14 22:53:48,571] Trial 21 finished with value: 0.5622 and parameters: {'lr': 0.006985731709661299, 'weight_decay': 0.001012535806344115, 'optimizer': 'AdamW'}. Best is trial 9 with value: 0.6495.\n",
      "[I 2026-01-14 22:56:22,546] Trial 22 finished with value: 0.6176 and parameters: {'lr': 0.004580262196218408, 'weight_decay': 0.00020258976259360231, 'optimizer': 'AdamW'}. Best is trial 9 with value: 0.6495.\n",
      "[I 2026-01-14 22:58:56,476] Trial 23 finished with value: 0.6286 and parameters: {'lr': 0.004448434114381778, 'weight_decay': 0.00021070293442876088, 'optimizer': 'AdamW'}. Best is trial 9 with value: 0.6495.\n",
      "[I 2026-01-14 22:59:02,639] Trial 24 pruned. \n",
      "[I 2026-01-14 22:59:18,956] Trial 25 pruned. \n",
      "[I 2026-01-14 22:59:25,100] Trial 26 pruned. \n",
      "[I 2026-01-14 22:59:36,467] Trial 27 pruned. \n",
      "[I 2026-01-14 22:59:42,574] Trial 28 pruned. \n",
      "[I 2026-01-14 22:59:48,794] Trial 29 pruned. \n",
      "[I 2026-01-14 22:59:55,072] Trial 30 pruned. \n",
      "[I 2026-01-14 23:00:01,198] Trial 31 pruned. \n",
      "[I 2026-01-14 23:00:07,379] Trial 32 pruned. \n",
      "[I 2026-01-14 23:00:13,626] Trial 33 pruned. \n",
      "[I 2026-01-14 23:00:19,745] Trial 34 pruned. \n",
      "[I 2026-01-14 23:00:25,909] Trial 35 pruned. \n",
      "[I 2026-01-14 23:00:42,344] Trial 36 pruned. \n",
      "[I 2026-01-14 23:00:48,466] Trial 37 pruned. \n",
      "[I 2026-01-14 23:00:59,706] Trial 38 pruned. \n",
      "[I 2026-01-14 23:01:05,921] Trial 39 pruned. \n",
      "[I 2026-01-14 23:01:12,215] Trial 40 pruned. \n",
      "[I 2026-01-14 23:01:18,314] Trial 41 pruned. \n",
      "[I 2026-01-14 23:01:34,716] Trial 42 pruned. \n",
      "[I 2026-01-14 23:01:40,935] Trial 43 pruned. \n",
      "[I 2026-01-14 23:01:52,274] Trial 44 pruned. \n",
      "[I 2026-01-14 23:02:03,536] Trial 45 pruned. \n",
      "[I 2026-01-14 23:02:19,944] Trial 46 pruned. \n",
      "[I 2026-01-14 23:02:31,227] Trial 47 pruned. \n",
      "[I 2026-01-14 23:04:58,592] Trial 48 finished with value: 0.6039 and parameters: {'lr': 0.006432784332651384, 'weight_decay': 8.035677372195e-05, 'optimizer': 'AdamW'}. Best is trial 9 with value: 0.6495.\n",
      "[I 2026-01-14 23:05:03,933] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'lr': 0.010406771084612782, 'weight_decay': 2.572424508783154e-05, 'optimizer': 'AdamW'}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # 1. 定义搜索空间\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"AdamW\"])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transform_train = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "    train_set = datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    test_set = datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    train_loader = DataLoader(train_set, batch_size=1280, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=256)\n",
    "    # 2. 初始化模型与优化器\n",
    "    model = SimpleCNN().to(device)\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # 3. 训练循环 (简化版，建议跑 20-30 epoch)\n",
    "    for epoch in range(30):\n",
    "        train_epoch(model, train_loader, optimizer, device)\n",
    "        val_acc = eval_epoch(model, test_loader, device)\n",
    "\n",
    "        # 允许提前停止（剪枝），节省时间\n",
    "        trial.report(val_acc, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_acc\n",
    "\n",
    "\n",
    "# 4. 启动寻优\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)  # 跑 50 组不同的配置\n",
    "\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pacr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
