{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3065b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from models import SimpleCNN\n",
    "import orjson\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11b4d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2. PACR Loss (batch-wise)\n",
    "# -------------------------\n",
    "def pacr_loss(features, labels):\n",
    "    \"\"\"\n",
    "    features: (B, D)\n",
    "    labels:   (B,)\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    num_classes = labels.max().item() + 1\n",
    "    eps = 1e-8\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        idx = labels == c\n",
    "        if idx.sum() < 2:\n",
    "            continue\n",
    "        z = features[idx]  # (Nc, D)\n",
    "        mean = z.mean(dim=0, keepdim=True)\n",
    "        loss += ((z - mean) ** 2).sum(dim=1).mean()\n",
    "\n",
    "    return loss / (num_classes + eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a047e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device, lambda_pacr=0.0):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total = 0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, feats = model(x, return_feat=True)\n",
    "        loss_ce = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss = loss_ce\n",
    "        if lambda_pacr > 0:\n",
    "            loss_pacr = pacr_loss(feats, y)\n",
    "            loss = loss + lambda_pacr * loss_pacr\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return total_loss / total, total_correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58710a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_correct, total = 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return total_correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21956107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-15 15:35:41,613] A new study created in memory with name: no-name-1778a676-8ff1-443b-a437-766c162140e3\n",
      "[I 2026-01-15 15:37:59,648] Trial 0 finished with value: 0.4324 and parameters: {'lr': 0.00045376786307441055, 'weight_decay': 0.0048668144034761055, 'lam': 0.005946278155930743}. Best is trial 0 with value: 0.4324.\n",
      "[I 2026-01-15 15:40:15,743] Trial 1 finished with value: 0.3274 and parameters: {'lr': 0.00010604588140671676, 'weight_decay': 0.002893251132734951, 'lam': 0.02786768939628681}. Best is trial 0 with value: 0.4324.\n",
      "[I 2026-01-15 15:42:31,020] Trial 2 finished with value: 0.1 and parameters: {'lr': 0.038205284430511084, 'weight_decay': 0.0013048678291405506, 'lam': 0.024650690215215067}. Best is trial 0 with value: 0.4324.\n",
      "[I 2026-01-15 15:44:43,554] Trial 3 finished with value: 0.1 and parameters: {'lr': 0.015874792905460153, 'weight_decay': 0.0008361187154794568, 'lam': 0.0018888587728558227}. Best is trial 0 with value: 0.4324.\n",
      "[I 2026-01-15 15:46:57,082] Trial 4 finished with value: 0.1 and parameters: {'lr': 0.046023846531248164, 'weight_decay': 7.65497370820669e-06, 'lam': 0.05088354008130608}. Best is trial 0 with value: 0.4324.\n",
      "[I 2026-01-15 15:47:02,860] Trial 5 pruned. \n",
      "[I 2026-01-15 15:47:08,748] Trial 6 pruned. \n",
      "[I 2026-01-15 15:49:27,860] Trial 7 finished with value: 0.5565 and parameters: {'lr': 0.021774441072643945, 'weight_decay': 1.4930343740797244e-05, 'lam': 0.0009121848063744826}. Best is trial 7 with value: 0.5565.\n",
      "[I 2026-01-15 15:49:33,500] Trial 8 pruned. \n",
      "[I 2026-01-15 15:51:50,236] Trial 9 finished with value: 0.6507 and parameters: {'lr': 0.006055927111592201, 'weight_decay': 0.0004358197533034887, 'lam': 0.08534858143576182}. Best is trial 9 with value: 0.6507.\n",
      "[I 2026-01-15 15:54:06,302] Trial 10 finished with value: 0.6275 and parameters: {'lr': 0.00376450713672314, 'weight_decay': 0.00012812416609902814, 'lam': 0.09832409017580186}. Best is trial 9 with value: 0.6507.\n",
      "[I 2026-01-15 15:56:19,801] Trial 11 finished with value: 0.62 and parameters: {'lr': 0.002797418906455845, 'weight_decay': 0.00014579084618935513, 'lam': 0.09910872199484076}. Best is trial 9 with value: 0.6507.\n",
      "[I 2026-01-15 15:58:38,913] Trial 12 finished with value: 0.6442 and parameters: {'lr': 0.004516930701444829, 'weight_decay': 0.00011547156382679792, 'lam': 0.01026016221550532}. Best is trial 9 with value: 0.6507.\n",
      "[I 2026-01-15 16:00:55,823] Trial 13 finished with value: 0.495 and parameters: {'lr': 0.004357032316105872, 'weight_decay': 1.3423366162135344e-06, 'lam': 0.008222328682344614}. Best is trial 9 with value: 0.6507.\n",
      "[I 2026-01-15 16:01:14,821] Trial 14 pruned. \n",
      "[I 2026-01-15 16:01:20,386] Trial 15 pruned. \n",
      "[I 2026-01-15 16:01:30,358] Trial 16 pruned. \n",
      "[I 2026-01-15 16:01:35,953] Trial 17 pruned. \n",
      "[I 2026-01-15 16:01:41,432] Trial 18 pruned. \n",
      "[I 2026-01-15 16:01:56,005] Trial 19 pruned. \n",
      "[I 2026-01-15 16:02:10,652] Trial 20 pruned. \n",
      "[I 2026-01-15 16:04:26,558] Trial 21 finished with value: 0.6109 and parameters: {'lr': 0.003632337475620804, 'weight_decay': 0.00010769383762495755, 'lam': 0.08363309615821046}. Best is trial 9 with value: 0.6507.\n",
      "[I 2026-01-15 16:04:32,046] Trial 22 pruned. \n",
      "[I 2026-01-15 16:04:37,498] Trial 23 pruned. \n",
      "[I 2026-01-15 16:05:09,387] Trial 24 pruned. \n",
      "[I 2026-01-15 16:05:14,850] Trial 25 pruned. \n",
      "[I 2026-01-15 16:05:29,101] Trial 26 pruned. \n",
      "[I 2026-01-15 16:06:05,347] Trial 27 pruned. \n",
      "[I 2026-01-15 16:06:10,880] Trial 28 pruned. \n",
      "[I 2026-01-15 16:08:24,905] Trial 29 finished with value: 0.674 and parameters: {'lr': 0.005494085007830311, 'weight_decay': 2.9335456707750783e-05, 'lam': 0.005151969339600816}. Best is trial 29 with value: 0.674.\n",
      "[I 2026-01-15 16:08:30,443] Trial 30 pruned. \n",
      "[I 2026-01-15 16:08:40,302] Trial 31 pruned. \n",
      "[I 2026-01-15 16:08:50,221] Trial 32 pruned. \n",
      "[I 2026-01-15 16:08:55,665] Trial 33 pruned. \n",
      "[I 2026-01-15 16:09:10,073] Trial 34 pruned. \n",
      "[I 2026-01-15 16:09:15,597] Trial 35 pruned. \n",
      "[I 2026-01-15 16:09:21,097] Trial 36 pruned. \n",
      "[I 2026-01-15 16:09:26,600] Trial 37 pruned. \n",
      "[I 2026-01-15 16:09:32,130] Trial 38 pruned. \n",
      "[I 2026-01-15 16:09:37,744] Trial 39 pruned. \n",
      "[I 2026-01-15 16:09:43,369] Trial 40 pruned. \n",
      "[I 2026-01-15 16:09:58,123] Trial 41 pruned. \n",
      "[I 2026-01-15 16:10:03,642] Trial 42 pruned. \n",
      "[I 2026-01-15 16:10:22,727] Trial 43 pruned. \n",
      "[I 2026-01-15 16:10:37,380] Trial 44 pruned. \n",
      "[I 2026-01-15 16:10:42,873] Trial 45 pruned. \n",
      "[I 2026-01-15 16:10:48,503] Trial 46 pruned. \n",
      "[I 2026-01-15 16:10:58,488] Trial 47 pruned. \n",
      "[I 2026-01-15 16:11:13,273] Trial 48 pruned. \n",
      "[I 2026-01-15 16:11:18,825] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'lr': 0.005494085007830311, 'weight_decay': 2.9335456707750783e-05, 'lam': 0.005151969339600816}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # 1. 定义搜索空间\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    lam = trial.suggest_float(\"lam\", 1e-4, 1e-1, log=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transform_train = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "    train_set = datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    test_set = datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    train_loader = DataLoader(train_set, batch_size=1280, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=256)\n",
    "    # 2. 初始化模型与优化器\n",
    "    model = SimpleCNN().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # 3. 训练循环 (简化版，建议跑 20-30 epoch)\n",
    "    for epoch in range(30):\n",
    "        train_epoch(model, train_loader, optimizer, device, lam)\n",
    "        val_acc = eval_epoch(model, test_loader, device)\n",
    "\n",
    "        # 允许提前停止（剪枝），节省时间\n",
    "        trial.report(val_acc, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_acc\n",
    "\n",
    "\n",
    "# 4. 启动寻优\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)  # 跑 50 组不同的配置\n",
    "\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pacr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
